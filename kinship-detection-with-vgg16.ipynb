{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jayesh/Documents/ML&DL_stuff/NorthEastern_SMILE/recognizing-faces-in-the-wild'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "os.getcwd()\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necassary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/rcmalli/keras-vggface.git\n",
      "  Cloning https://github.com/rcmalli/keras-vggface.git to /tmp/pip-req-build-8p4eumn8\n",
      "  Running command git clone -q https://github.com/rcmalli/keras-vggface.git /tmp/pip-req-build-8p4eumn8\n",
      "Requirement already satisfied (use --upgrade to upgrade): keras-vggface==0.5 from git+https://github.com/rcmalli/keras-vggface.git in /home/jayesh/.local/lib/python3.6/site-packages\n",
      "Requirement already satisfied: h5py in /home/jayesh/.local/lib/python3.6/site-packages (from keras-vggface==0.5) (2.9.0)\n",
      "Requirement already satisfied: keras in /home/jayesh/.local/lib/python3.6/site-packages (from keras-vggface==0.5) (2.2.4)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/jayesh/.local/lib/python3.6/site-packages (from keras-vggface==0.5) (1.16.2)\n",
      "Requirement already satisfied: pillow in /usr/lib/python3/dist-packages (from keras-vggface==0.5) (5.1.0)\n",
      "Requirement already satisfied: pyyaml in /home/jayesh/.local/lib/python3.6/site-packages (from keras-vggface==0.5) (3.13)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/jayesh/.local/lib/python3.6/site-packages (from keras-vggface==0.5) (1.2.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/jayesh/.local/lib/python3.6/site-packages (from keras-vggface==0.5) (1.12.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/jayesh/.local/lib/python3.6/site-packages (from keras->keras-vggface==0.5) (1.0.7)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/jayesh/.local/lib/python3.6/site-packages (from keras->keras-vggface==0.5) (1.0.6)\n",
      "Building wheels for collected packages: keras-vggface\n",
      "  Building wheel for keras-vggface (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-w48h49rz/wheels/36/07/46/06c25ce8e9cd396dabe151ea1d8a2bc28dafcb11321c1f3a6d\n",
      "Successfully built keras-vggface\n"
     ]
    }
   ],
   "source": [
    "!pip install --user git+https://github.com/rcmalli/keras-vggface.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_obtain_input_shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-95cc339f7928>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#from keras_vggface.utils import preprocess_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#from keras_vggface.vggface import VGGFace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimagenet_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_obtain_input_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_obtain_input_shape'"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "from random import choice, sample\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers import Input, Dense, Flatten, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract, Add, Conv2D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "#from keras_vggface.utils import preprocess_input\n",
    "#from keras_vggface.vggface import VGGFace\n",
    "from keras.applications.imagenet_utils import _obtain_input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = \"train_relationships.csv\"\n",
    "train_folders_path = \"/train/\"\n",
    "val_famillies = \"F09\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = glob(train_folders_path + \"*/*/*.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation set consists only family 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = [x for x in all_images if val_famillies not in x]\n",
    "val_images = [x for x in all_images if val_famillies in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_person_to_images_map = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl = [x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2] for x in all_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in train_images:\n",
    "    train_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n",
    "\n",
    "val_person_to_images_map = defaultdict(list)\n",
    "\n",
    "for x in val_images:\n",
    "    val_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n",
    "\n",
    "relationships = pd.read_csv(train_file_path)\n",
    "relationships = list(zip(relationships.p1.values, relationships.p2.values))\n",
    "relationships = [x for x in relationships if x[0] in ppl and x[1] in ppl]\n",
    "\n",
    "train = [x for x in relationships if val_famillies not in x[0]]\n",
    "val = [x for x in relationships if val_famillies in x[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'VGGFace' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-ba42d6304fef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0mcallbacks_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_on_plateau\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseline_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-ba42d6304fef>\u001b[0m in \u001b[0;36mbaseline_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0minput_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mbase_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGGFace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'resnet50'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'VGGFace' is not defined"
     ]
    }
   ],
   "source": [
    "def read_img(path):\n",
    "    img = cv2.imread(path)\n",
    "    img = np.array(img).astype(np.float)\n",
    "    return preprocess_input(img, version=2)\n",
    "\n",
    "\n",
    "def gen(list_tuples, person_to_images_map, batch_size=16):\n",
    "    ppl = list(person_to_images_map.keys())\n",
    "    while True:\n",
    "        batch_tuples = sample(list_tuples, batch_size // 2)\n",
    "        labels = [1] * len(batch_tuples)\n",
    "        while len(batch_tuples) < batch_size:\n",
    "            p1 = choice(ppl)\n",
    "            p2 = choice(ppl)\n",
    "\n",
    "            if p1 != p2 and (p1, p2) not in list_tuples and (p2, p1) not in list_tuples:\n",
    "                batch_tuples.append((p1, p2))\n",
    "                labels.append(0)\n",
    "\n",
    "        for x in batch_tuples:\n",
    "            if not len(person_to_images_map[x[0]]):\n",
    "                print(x[0])\n",
    "\n",
    "        X1 = [choice(person_to_images_map[x[0]]) for x in batch_tuples]\n",
    "        X1 = np.array([read_img(x) for x in X1])\n",
    "\n",
    "        X2 = [choice(person_to_images_map[x[1]]) for x in batch_tuples]\n",
    "        X2 = np.array([read_img(x) for x in X2])\n",
    "\n",
    "        yield [X1, X2], labels\n",
    "\n",
    "\n",
    "def baseline_model():\n",
    "    input_1 = Input(shape=(224, 224, 3))\n",
    "    input_2 = Input(shape=(224, 224, 3))\n",
    "\n",
    "    base_model = VGGFace(model='resnet50', include_top=False)\n",
    "\n",
    "    for layer in base_model.layers[:-3]:\n",
    "        layer.trainable = True\n",
    "\n",
    "    x1 = base_model(input_1)\n",
    "    x2 = base_model(input_2)\n",
    "\n",
    "\n",
    "    merged_add = Add()([x1, x2])\n",
    "    merged_sub = Subtract()([x1,x2])\n",
    "    \n",
    "    merged_add = Conv2D(100 , [1,1] )(merged_add)\n",
    "    merged_sub = Conv2D(100 , [1,1] )(merged_sub)\n",
    "    \n",
    "    merged = Concatenate(axis=-1)([merged_add, merged_sub])\n",
    "    \n",
    "    merged = Flatten()(merged)\n",
    "\n",
    "    merged = Dense(100, activation=\"relu\")(merged)\n",
    "#     merged = Dropout(0.3)(merged)\n",
    "#     merged = Dense(25, activation=\"relu\")(merged)\n",
    "#     merged = Dropout(0.3)(merged)\n",
    "#     out = Dense(1, activation=\"sigmoid\")(merged)\n",
    "\n",
    "    model = Model([input_1, input_2])\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adam(0.00001))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "file_path = \"vgg_face.h5\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "reduce_on_plateau = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", factor=0.1, patience=20, verbose=1)\n",
    "\n",
    "callbacks_list = [checkpoint, reduce_on_plateau]\n",
    "\n",
    "model = baseline_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/engine/training_generator.py:47: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 105s 526ms/step - loss: 1.2316 - acc: 0.5394 - val_loss: 0.7537 - val_acc: 0.5994\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.59937, saving model to vgg_face.h5\n",
      "Epoch 2/130\n",
      "200/200 [==============================] - 78s 390ms/step - loss: 0.8789 - acc: 0.5725 - val_loss: 0.7092 - val_acc: 0.6088\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.59937 to 0.60875, saving model to vgg_face.h5\n",
      "Epoch 3/130\n",
      "200/200 [==============================] - 78s 391ms/step - loss: 0.7836 - acc: 0.5741 - val_loss: 0.6885 - val_acc: 0.6081\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.60875\n",
      "Epoch 4/130\n",
      "200/200 [==============================] - 79s 395ms/step - loss: 0.6929 - acc: 0.6081 - val_loss: 0.6612 - val_acc: 0.6044\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.60875\n",
      "Epoch 5/130\n",
      "200/200 [==============================] - 79s 395ms/step - loss: 0.7112 - acc: 0.5900 - val_loss: 0.6429 - val_acc: 0.6238\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.60875 to 0.62375, saving model to vgg_face.h5\n",
      "Epoch 6/130\n",
      "200/200 [==============================] - 78s 389ms/step - loss: 0.6717 - acc: 0.6138 - val_loss: 0.6252 - val_acc: 0.6238\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.62375\n",
      "Epoch 7/130\n",
      "200/200 [==============================] - 79s 396ms/step - loss: 0.6386 - acc: 0.6375 - val_loss: 0.6033 - val_acc: 0.6775\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.62375 to 0.67750, saving model to vgg_face.h5\n",
      "Epoch 8/130\n",
      "200/200 [==============================] - 77s 386ms/step - loss: 0.6400 - acc: 0.6450 - val_loss: 0.6043 - val_acc: 0.6594\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.67750\n",
      "Epoch 9/130\n",
      "200/200 [==============================] - 79s 394ms/step - loss: 0.6384 - acc: 0.6381 - val_loss: 0.5996 - val_acc: 0.6538\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.67750\n",
      "Epoch 10/130\n",
      "200/200 [==============================] - 79s 394ms/step - loss: 0.6118 - acc: 0.6544 - val_loss: 0.5943 - val_acc: 0.6650\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.67750\n",
      "Epoch 11/130\n",
      "200/200 [==============================] - 79s 393ms/step - loss: 0.6009 - acc: 0.6553 - val_loss: 0.5930 - val_acc: 0.6813\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.67750 to 0.68125, saving model to vgg_face.h5\n",
      "Epoch 12/130\n",
      "200/200 [==============================] - 77s 385ms/step - loss: 0.6009 - acc: 0.6647 - val_loss: 0.6018 - val_acc: 0.6975\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.68125 to 0.69750, saving model to vgg_face.h5\n",
      "Epoch 13/130\n",
      "200/200 [==============================] - 78s 388ms/step - loss: 0.5913 - acc: 0.6831 - val_loss: 0.5845 - val_acc: 0.6894\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.69750\n",
      "Epoch 14/130\n",
      "200/200 [==============================] - 78s 390ms/step - loss: 0.5743 - acc: 0.6916 - val_loss: 0.5660 - val_acc: 0.7006\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.69750 to 0.70063, saving model to vgg_face.h5\n",
      "Epoch 15/130\n",
      "200/200 [==============================] - 76s 380ms/step - loss: 0.5746 - acc: 0.6916 - val_loss: 0.5648 - val_acc: 0.7094\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.70063 to 0.70937, saving model to vgg_face.h5\n",
      "Epoch 16/130\n",
      "200/200 [==============================] - 76s 381ms/step - loss: 0.5633 - acc: 0.7006 - val_loss: 0.5835 - val_acc: 0.7031\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.70937\n",
      "Epoch 17/130\n",
      "200/200 [==============================] - 78s 390ms/step - loss: 0.5404 - acc: 0.7253 - val_loss: 0.5594 - val_acc: 0.6994\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.70937\n",
      "Epoch 18/130\n",
      "200/200 [==============================] - 78s 388ms/step - loss: 0.5305 - acc: 0.7247 - val_loss: 0.5452 - val_acc: 0.7188\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.70937 to 0.71875, saving model to vgg_face.h5\n",
      "Epoch 19/130\n",
      "200/200 [==============================] - 75s 374ms/step - loss: 0.5213 - acc: 0.7369 - val_loss: 0.5285 - val_acc: 0.7338\n",
      "\n",
      "Epoch 00019: val_acc improved from 0.71875 to 0.73375, saving model to vgg_face.h5\n",
      "Epoch 20/130\n",
      "200/200 [==============================] - 75s 377ms/step - loss: 0.5266 - acc: 0.7284 - val_loss: 0.5316 - val_acc: 0.7331\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.73375\n",
      "Epoch 21/130\n",
      "200/200 [==============================] - 77s 386ms/step - loss: 0.5409 - acc: 0.7122 - val_loss: 0.5342 - val_acc: 0.7156\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.73375\n",
      "Epoch 22/130\n",
      "200/200 [==============================] - 78s 389ms/step - loss: 0.5183 - acc: 0.7350 - val_loss: 0.5336 - val_acc: 0.7194\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.73375\n",
      "Epoch 23/130\n",
      "200/200 [==============================] - 77s 383ms/step - loss: 0.5325 - acc: 0.7291 - val_loss: 0.5299 - val_acc: 0.7356\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.73375 to 0.73562, saving model to vgg_face.h5\n",
      "Epoch 24/130\n",
      "200/200 [==============================] - 75s 373ms/step - loss: 0.5154 - acc: 0.7363 - val_loss: 0.5380 - val_acc: 0.7406\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.73562 to 0.74062, saving model to vgg_face.h5\n",
      "Epoch 25/130\n",
      "200/200 [==============================] - 75s 376ms/step - loss: 0.4977 - acc: 0.7550 - val_loss: 0.5134 - val_acc: 0.7375\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.74062\n",
      "Epoch 26/130\n",
      "200/200 [==============================] - 77s 387ms/step - loss: 0.5142 - acc: 0.7441 - val_loss: 0.5031 - val_acc: 0.7544\n",
      "\n",
      "Epoch 00026: val_acc improved from 0.74062 to 0.75438, saving model to vgg_face.h5\n",
      "Epoch 27/130\n",
      "200/200 [==============================] - 75s 375ms/step - loss: 0.4879 - acc: 0.7697 - val_loss: 0.5122 - val_acc: 0.7450\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.75438\n",
      "Epoch 28/130\n",
      "200/200 [==============================] - 77s 384ms/step - loss: 0.5014 - acc: 0.7600 - val_loss: 0.5218 - val_acc: 0.7381\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.75438\n",
      "Epoch 29/130\n",
      "200/200 [==============================] - 76s 381ms/step - loss: 0.4836 - acc: 0.7597 - val_loss: 0.5184 - val_acc: 0.7394\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.75438\n",
      "Epoch 30/130\n",
      "  8/200 [>.............................] - ETA: 1:23 - loss: 0.4876 - acc: 0.7344"
     ]
    }
   ],
   "source": [
    "model.fit_generator(gen(train, train_person_to_images_map, batch_size=16), use_multiprocessing=True,\n",
    "                    validation_data=gen(val, val_person_to_images_map, batch_size=16), epochs=130, verbose=1,\n",
    "                    workers = 4, callbacks=callbacks_list, steps_per_epoch=200, validation_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /home/jayesh/.local/lib/python3.6/site-packages (2.2.4)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/jayesh/.local/lib/python3.6/site-packages (from keras) (1.0.7)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/jayesh/.local/lib/python3.6/site-packages (from keras) (1.16.2)\n",
      "Requirement already satisfied: h5py in /home/jayesh/.local/lib/python3.6/site-packages (from keras) (2.9.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/jayesh/.local/lib/python3.6/site-packages (from keras) (1.12.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/jayesh/.local/lib/python3.6/site-packages (from keras) (1.0.6)\n",
      "Requirement already satisfied: pyyaml in /home/jayesh/.local/lib/python3.6/site-packages (from keras) (3.13)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/jayesh/.local/lib/python3.6/site-packages (from keras) (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --user keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "166it [01:22,  1.81it/s]\n"
     ]
    }
   ],
   "source": [
    "test_path = \"../input/test/\"\n",
    "\n",
    "\n",
    "def chunker(seq, size=32):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "submission = pd.read_csv('../input/sample_submission.csv')\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for batch in tqdm(chunker(submission.img_pair.values)):\n",
    "    X1 = [x.split(\"-\")[0] for x in batch]\n",
    "    X1 = np.array([read_img(test_path + x) for x in X1])\n",
    "\n",
    "    X2 = [x.split(\"-\")[1] for x in batch]\n",
    "    X2 = np.array([read_img(test_path + x) for x in X2])\n",
    "\n",
    "    pred = model.predict([X1, X2]).ravel().tolist()\n",
    "    predictions += pred\n",
    "\n",
    "submission['is_related'] = predictions\n",
    "\n",
    "submission.to_csv(\"vgg_face.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
